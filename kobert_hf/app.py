# FastAPI ë°±ì—”ë“œ ì„œë²„ - ë¬¸ì¥ ìˆœì„œ ì˜ˆì¸¡ ëª¨ë¸ API
# 2025-11-13, ê¹€ë³‘í˜„ ì‘ì„±

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from pydantic import BaseModel
from typing import List
import torch
import os
import sys

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ sys.pathì— ì¶”ê°€ (ëª¨ë“ˆ importë¥¼ ìœ„í•´)
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from kobert_tokenizer import KoBERTTokenizer
from src.models.sentence_order_model import SentenceOrderPredictor

# ==================== FastAPI ì•± ì´ˆê¸°í™” ====================

app = FastAPI(
    title="KoBERT ë¬¸ì¥ ìˆœì„œ ì˜ˆì¸¡ API",
    description="í•œêµ­ì–´ ë¬¸ì¥ë“¤ì˜ ì˜¬ë°”ë¥¸ ìˆœì„œë¥¼ ì˜ˆì¸¡í•˜ëŠ” API",
    version="1.0.0"
)

# CORS ì„¤ì • (í”„ë¡ íŠ¸ì—”ë“œ ì—°ê²°ì„ ìœ„í•´)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ëª¨ë“  ì¶œì²˜ í—ˆìš© (í”„ë¡œë•ì…˜ì—ì„œëŠ” íŠ¹ì • ë„ë©”ì¸ë§Œ í—ˆìš©)
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==================== ì „ì—­ ë³€ìˆ˜ ====================

# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ì „ì—­ìœ¼ë¡œ ë¡œë“œ (ì„œë²„ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ë¡œë“œ)
MODEL = None
TOKENIZER = None
DEVICE = None

# ==================== ìš”ì²­/ì‘ë‹µ ëª¨ë¸ ====================

class PredictRequest(BaseModel):
    """ë¬¸ì¥ ìˆœì„œ ì˜ˆì¸¡ ìš”ì²­"""
    sentences: List[str]

    class Config:
        json_schema_extra = {
            "example": {
                "sentences": [
                    "í•™êµì— ê°”ë‹¤",
                    "ì¼ì–´ë‚¬ë‹¤",
                    "ë°¥ì„ ë¨¹ì—ˆë‹¤",
                    "ì„¸ìˆ˜ë¥¼ í–ˆë‹¤"
                ]
            }
        }

class PredictResponse(BaseModel):
    """ë¬¸ì¥ ìˆœì„œ ì˜ˆì¸¡ ì‘ë‹µ"""
    original_sentences: List[str]  # ì…ë ¥ë°›ì€ ì„ì¸ ë¬¸ì¥ë“¤
    predicted_order: List[int]  # ì˜ˆì¸¡ëœ ìˆœì„œ (ì¸ë±ìŠ¤)
    sorted_sentences: List[str]  # ì •ë ¬ëœ ë¬¸ì¥ë“¤
    confidence_scores: List[List[float]]  # ê° ë¬¸ì¥ì˜ ìœ„ì¹˜ë³„ í™•ë¥ 

# ==================== ì„œë²„ ì‹œì‘/ì¢…ë£Œ ì´ë²¤íŠ¸ ====================

@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ"""
    global MODEL, TOKENIZER, DEVICE

    print("=" * 70)
    print("ğŸš€ FastAPI ì„œë²„ ì‹œì‘ ì¤‘...")
    print("=" * 70)

    # Device ì„¤ì •
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"âœ… Device: {DEVICE}")

    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    print("âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...")
    TOKENIZER = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')
    print("   í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ!")

    # ëª¨ë¸ ë¡œë“œ
    print("âœ… ëª¨ë¸ ë¡œë“œ ì¤‘...")
    MODEL = SentenceOrderPredictor(max_sentences=12, hidden_size=768, dropout=0.1).to(DEVICE)

    # í•™ìŠµëœ ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ
    model_path = "models/sentence_order_model_best.pt"
    if os.path.exists(model_path):
        MODEL.load_state_dict(torch.load(model_path, map_location=DEVICE))
        print(f"   âœ… í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")
    else:
        print(f"   âš ï¸  í•™ìŠµëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        print(f"   âš ï¸  ëœë¤ ê°€ì¤‘ì¹˜ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤ (ì •í™•ë„ ë‚®ìŒ)")

    MODEL.eval()  # í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •

    total_params = sum(p.numel() for p in MODEL.parameters())
    print(f"   ì „ì²´ íŒŒë¼ë¯¸í„°: {total_params:,}")
    print("=" * 70)
    print("âœ… ì„œë²„ ì¤€ë¹„ ì™„ë£Œ!")
    print("=" * 70)

# ==================== API ì—”ë“œí¬ì¸íŠ¸ ====================

@app.get("/")
async def root():
    """ë£¨íŠ¸ ê²½ë¡œ - í”„ë¡ íŠ¸ì—”ë“œ HTML ì œê³µ"""
    html_path = os.path.join(os.path.dirname(__file__), "static", "index.html")
    if os.path.exists(html_path):
        return FileResponse(html_path)
    return {"message": "KoBERT ë¬¸ì¥ ìˆœì„œ ì˜ˆì¸¡ API", "docs": "/docs"}

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    return {
        "status": "healthy",
        "model_loaded": MODEL is not None,
        "device": str(DEVICE)
    }

@app.post("/predict", response_model=PredictResponse)
async def predict_sentence_order(request: PredictRequest):
    """
    ë¬¸ì¥ ìˆœì„œ ì˜ˆì¸¡ API

    Args:
        request: ì„ì¸ ë¬¸ì¥ë“¤ì˜ ë¦¬ìŠ¤íŠ¸

    Returns:
        ì˜ˆì¸¡ëœ ìˆœì„œì™€ ì •ë ¬ëœ ë¬¸ì¥ë“¤
    """
    if MODEL is None or TOKENIZER is None:
        raise HTTPException(status_code=503, detail="ëª¨ë¸ì´ ì•„ì§ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

    sentences = request.sentences

    # ì…ë ¥ ê²€ì¦
    if not sentences:
        raise HTTPException(status_code=400, detail="ë¬¸ì¥ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")

    if len(sentences) > 7:
        raise HTTPException(status_code=400, detail="ìµœëŒ€ 7ê°œì˜ ë¬¸ì¥ê¹Œì§€ ì§€ì›í•©ë‹ˆë‹¤")

    try:
        # ëª¨ë¸ ì¶”ë¡ 
        # 2025-11-18, ê¹€ë³‘í˜„ ìˆ˜ì • - ì¤‘ë³µ ì„ íƒ ë°©ì§€ë¥¼ ìœ„í•œ greedy decoding êµ¬í˜„
        with torch.no_grad():
            # ê° ë¬¸ì¥ì„ BERTë¡œ ì¸ì½”ë”©
            num_sentences = len(sentences)
            sentence_embeddings = []

            for sent in sentences:
                inputs = TOKENIZER(
                    sent,
                    return_tensors='pt',
                    padding='max_length',
                    max_length=64,
                    truncation=True
                )
                input_ids = inputs['input_ids'].to(DEVICE)
                attention_mask = inputs['attention_mask'].to(DEVICE)

                # BERT ì¸ì½”ë”©
                outputs = MODEL.bert(input_ids=input_ids, attention_mask=attention_mask)
                sentence_emb = outputs.pooler_output  # [1, hidden_size]
                sentence_embeddings.append(sentence_emb)

            # ëª¨ë“  ë¬¸ì¥ ì„ë² ë”©ì„ ìŒ“ê¸°
            sentence_embeddings = torch.stack(sentence_embeddings, dim=1)  # [1, num_sentences, hidden_size]

            # ìˆœì„œ ì¸ì½”ë” ì‹¤í–‰
            encoder_outputs, _ = MODEL.sequence_encoder(sentence_embeddings)

            # Pointer Decoder - Greedy Decoding (ì¤‘ë³µ ë°©ì§€)
            batch_size = 1
            hidden_size = MODEL.pointer_decoder.hidden_size

            # LSTM ì´ˆê¸° ìƒíƒœ
            h0 = torch.zeros(1, batch_size, hidden_size).to(DEVICE)
            c0 = torch.zeros(1, batch_size, hidden_size).to(DEVICE)
            decoder_hidden = (h0, c0)

            # ì´ˆê¸° ì…ë ¥
            decoder_input = MODEL.pointer_decoder.initial_input.repeat(batch_size, 1, 1)

            # ì´ë¯¸ ì„ íƒëœ ë¬¸ì¥ ë§ˆìŠ¤í‚¹
            mask = torch.zeros(batch_size, num_sentences, dtype=torch.bool).to(DEVICE)

            selected_indices_list = []
            all_probabilities = []

            # ê° ìŠ¤í…ë§ˆë‹¤ ë¬¸ì¥ ì„ íƒ
            for step in range(num_sentences):
                # LSTM ì‹¤í–‰
                decoder_output, decoder_hidden = MODEL.pointer_decoder.lstm(decoder_input, decoder_hidden)

                # Attention ì ìˆ˜ ê³„ì‚°
                scores = MODEL.pointer_decoder.attention(decoder_hidden[0], encoder_outputs)

                # ë§ˆìŠ¤í‚¹ ì ìš© (ì´ë¯¸ ì„ íƒëœ ë¬¸ì¥ ì œì™¸)
                masked_scores = scores.masked_fill(mask, -1e9)

                # Softmaxë¡œ í™•ë¥  ê³„ì‚°
                probabilities = torch.softmax(masked_scores, dim=1)
                all_probabilities.append(probabilities.squeeze(0).cpu().tolist())

                # ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ ë¬¸ì¥ ì„ íƒ
                predicted_index = torch.argmax(probabilities, dim=1)
                selected_indices_list.append(predicted_index.item())

                # ì„ íƒëœ ë¬¸ì¥ ë§ˆìŠ¤í‚¹
                mask.scatter_(dim=1, index=predicted_index.unsqueeze(1), value=True)

                # ë‹¤ìŒ ì…ë ¥ ì¤€ë¹„
                decoder_input = torch.gather(
                    encoder_outputs,
                    1,
                    predicted_index.view(batch_size, 1, 1).repeat(1, 1, hidden_size)
                )

            # ê²°ê³¼ ì •ë¦¬
            sorted_sentences = [sentences[idx] for idx in selected_indices_list]
            predicted_order = [0] * num_sentences
            for position, sentence_idx in enumerate(selected_indices_list):
                predicted_order[sentence_idx] = position

            return PredictResponse(
                original_sentences=sentences,
                predicted_order=predicted_order,
                sorted_sentences=sorted_sentences,
                confidence_scores=all_probabilities
            )

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ëª¨ë¸ ì¶”ë¡  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

# ==================== ì •ì  íŒŒì¼ ì œê³µ ====================

# static í´ë”ê°€ ìˆìœ¼ë©´ ì •ì  íŒŒì¼ ì œê³µ
static_dir = os.path.join(os.path.dirname(__file__), "static")
if os.path.exists(static_dir):
    app.mount("/static", StaticFiles(directory=static_dir), name="static")

# ==================== ì„œë²„ ì‹¤í–‰ ====================

if __name__ == "__main__":
    import uvicorn

    print("=" * 70)
    print("ğŸš€ ì„œë²„ ì‹œì‘")
    print("=" * 70)
    print("ğŸ“ URL: http://localhost:8000")
    print("ğŸ“– API ë¬¸ì„œ: http://localhost:8000/docs")
    print("=" * 70)

    uvicorn.run(app, host="0.0.0.0", port=8000)
