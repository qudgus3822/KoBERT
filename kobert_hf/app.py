# FastAPI ë°±ì—”ë“œ ì„œë²„ - ë¬¸ì¥ ìˆœì„œ ì˜ˆì¸¡ ëª¨ë¸ API
# 2025-11-13, ê¹€ë³‘í˜„ ì‘ì„±

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from pydantic import BaseModel
from typing import List
import torch
import os
import sys

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ sys.pathì— ì¶”ê°€ (ëª¨ë“ˆ importë¥¼ ìœ„í•´)
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from kobert_tokenizer import KoBERTTokenizer
from src.models.sentence_order_model import SentenceOrderPredictor

# ==================== FastAPI ì•± ì´ˆê¸°í™” ====================

app = FastAPI(
    title="KoBERT ë¬¸ì¥ ìˆœì„œ ì˜ˆì¸¡ API",
    description="í•œêµ­ì–´ ë¬¸ì¥ë“¤ì˜ ì˜¬ë°”ë¥¸ ìˆœì„œë¥¼ ì˜ˆì¸¡í•˜ëŠ” API",
    version="1.0.0"
)

# CORS ì„¤ì • (í”„ë¡ íŠ¸ì—”ë“œ ì—°ê²°ì„ ìœ„í•´)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ëª¨ë“  ì¶œì²˜ í—ˆìš© (í”„ë¡œë•ì…˜ì—ì„œëŠ” íŠ¹ì • ë„ë©”ì¸ë§Œ í—ˆìš©)
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==================== ì „ì—­ ë³€ìˆ˜ ====================

# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ì „ì—­ìœ¼ë¡œ ë¡œë“œ (ì„œë²„ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ë¡œë“œ)
MODEL = None
TOKENIZER = None
DEVICE = None

# ==================== ìš”ì²­/ì‘ë‹µ ëª¨ë¸ ====================

class PredictRequest(BaseModel):
    """ë¬¸ì¥ ìˆœì„œ ì˜ˆì¸¡ ìš”ì²­"""
    sentences: List[str]

    class Config:
        json_schema_extra = {
            "example": {
                "sentences": [
                    "í•™êµì— ê°”ë‹¤",
                    "ì¼ì–´ë‚¬ë‹¤",
                    "ë°¥ì„ ë¨¹ì—ˆë‹¤",
                    "ì„¸ìˆ˜ë¥¼ í–ˆë‹¤"
                ]
            }
        }

class PredictResponse(BaseModel):
    """ë¬¸ì¥ ìˆœì„œ ì˜ˆì¸¡ ì‘ë‹µ"""
    original_sentences: List[str]  # ì…ë ¥ë°›ì€ ì„ì¸ ë¬¸ì¥ë“¤
    predicted_order: List[int]  # ì˜ˆì¸¡ëœ ìˆœì„œ (ì¸ë±ìŠ¤)
    sorted_sentences: List[str]  # ì •ë ¬ëœ ë¬¸ì¥ë“¤
    confidence_scores: List[List[float]]  # ê° ë¬¸ì¥ì˜ ìœ„ì¹˜ë³„ í™•ë¥ 

# ==================== ì„œë²„ ì‹œì‘/ì¢…ë£Œ ì´ë²¤íŠ¸ ====================

@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ"""
    global MODEL, TOKENIZER, DEVICE

    print("=" * 70)
    print("ğŸš€ FastAPI ì„œë²„ ì‹œì‘ ì¤‘...")
    print("=" * 70)

    # Device ì„¤ì •
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"âœ… Device: {DEVICE}")

    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    print("âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...")
    TOKENIZER = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')
    print("   í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ!")

    # ëª¨ë¸ ë¡œë“œ
    print("âœ… ëª¨ë¸ ë¡œë“œ ì¤‘...")
    MODEL = SentenceOrderPredictor(max_sentences=12, hidden_size=768, dropout=0.1).to(DEVICE)

    # í•™ìŠµëœ ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ
    model_path = "models/sentence_order_model_best.pt"
    if os.path.exists(model_path):
        MODEL.load_state_dict(torch.load(model_path, map_location=DEVICE))
        print(f"   âœ… í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")
    else:
        print(f"   âš ï¸  í•™ìŠµëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        print(f"   âš ï¸  ëœë¤ ê°€ì¤‘ì¹˜ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤ (ì •í™•ë„ ë‚®ìŒ)")

    MODEL.eval()  # í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •

    total_params = sum(p.numel() for p in MODEL.parameters())
    print(f"   ì „ì²´ íŒŒë¼ë¯¸í„°: {total_params:,}")
    print("=" * 70)
    print("âœ… ì„œë²„ ì¤€ë¹„ ì™„ë£Œ!")
    print("=" * 70)

# ==================== API ì—”ë“œí¬ì¸íŠ¸ ====================

@app.get("/")
async def root():
    """ë£¨íŠ¸ ê²½ë¡œ - í”„ë¡ íŠ¸ì—”ë“œ HTML ì œê³µ"""
    html_path = os.path.join(os.path.dirname(__file__), "static", "index.html")
    if os.path.exists(html_path):
        return FileResponse(html_path)
    return {"message": "KoBERT ë¬¸ì¥ ìˆœì„œ ì˜ˆì¸¡ API", "docs": "/docs"}

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    return {
        "status": "healthy",
        "model_loaded": MODEL is not None,
        "device": str(DEVICE)
    }

@app.post("/predict", response_model=PredictResponse)
async def predict_sentence_order(request: PredictRequest):
    """
    ë¬¸ì¥ ìˆœì„œ ì˜ˆì¸¡ API

    Args:
        request: ì„ì¸ ë¬¸ì¥ë“¤ì˜ ë¦¬ìŠ¤íŠ¸

    Returns:
        ì˜ˆì¸¡ëœ ìˆœì„œì™€ ì •ë ¬ëœ ë¬¸ì¥ë“¤
    """
    if MODEL is None or TOKENIZER is None:
        raise HTTPException(status_code=503, detail="ëª¨ë¸ì´ ì•„ì§ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

    sentences = request.sentences

    # ì…ë ¥ ê²€ì¦
    if not sentences:
        raise HTTPException(status_code=400, detail="ë¬¸ì¥ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")

    if len(sentences) > 12:
        raise HTTPException(status_code=400, detail="ìµœëŒ€ 12ê°œì˜ ë¬¸ì¥ê¹Œì§€ ì§€ì›í•©ë‹ˆë‹¤")

    try:
        # ëª¨ë¸ ì¶”ë¡ 
        with torch.no_grad():
            # ê° ë¬¸ì¥ì„ í† í°í™”
            input_ids_list = []
            attention_mask_list = []

            for sent in sentences:
                inputs = TOKENIZER(
                    sent,
                    return_tensors='pt',
                    padding='max_length',
                    max_length=64,
                    truncation=True
                )
                input_ids_list.append(inputs['input_ids'].to(DEVICE))
                attention_mask_list.append(inputs['attention_mask'].to(DEVICE))

            # ëª¨ë¸ ì‹¤í–‰
            logits = MODEL(input_ids_list, attention_mask_list)

            # í™•ë¥ ë¡œ ë³€í™˜ (ì†Œí”„íŠ¸ë§¥ìŠ¤)
            probabilities = torch.softmax(logits, dim=-1).squeeze(0)  # [num_sentences, num_sentences]

            # ê° ë¬¸ì¥ì— ëŒ€í•´ ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ ìœ„ì¹˜ ì„ íƒ
            predicted_positions = torch.argmax(logits, dim=-1).squeeze(0)  # [num_sentences]

            # ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            predicted_order = predicted_positions.cpu().tolist()
            confidence_scores = probabilities.cpu().tolist()

            # ì˜ˆì¸¡ëœ ìˆœì„œëŒ€ë¡œ ë¬¸ì¥ ì •ë ¬
            # 2025-11-13, ê¹€ë³‘í˜„ ìˆ˜ì • - ì¤‘ë³µ ìœ„ì¹˜ ì²˜ë¦¬ ê°œì„  (ëª¨ë“  ë¬¸ì¥ í‘œì‹œ)
            # predicted_order[i] = j ëŠ” "ië²ˆì§¸ ë¬¸ì¥ì´ jë²ˆì§¸ ìœ„ì¹˜ì— ì™€ì•¼ í•œë‹¤"ëŠ” ì˜ë¯¸

            # ìœ„ì¹˜ë³„ë¡œ ë¬¸ì¥ë“¤ì„ ê·¸ë£¹í™” (ê°™ì€ ìœ„ì¹˜ì— ì—¬ëŸ¬ ë¬¸ì¥ì´ ì˜¬ ìˆ˜ ìˆìŒ)
            position_to_sentences = {}
            for i, pos in enumerate(predicted_order):
                if pos not in position_to_sentences:
                    position_to_sentences[pos] = []
                position_to_sentences[pos].append(sentences[i])

            # ìœ„ì¹˜ ìˆœì„œëŒ€ë¡œ ì •ë ¬í•˜ì—¬ ìµœì¢… ë¦¬ìŠ¤íŠ¸ ìƒì„±
            sorted_sentences = []
            for pos in sorted(position_to_sentences.keys()):
                sorted_sentences.extend(position_to_sentences[pos])

            return PredictResponse(
                original_sentences=sentences,
                predicted_order=predicted_order,
                sorted_sentences=sorted_sentences,
                confidence_scores=confidence_scores
            )

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ëª¨ë¸ ì¶”ë¡  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

# ==================== ì •ì  íŒŒì¼ ì œê³µ ====================

# static í´ë”ê°€ ìˆìœ¼ë©´ ì •ì  íŒŒì¼ ì œê³µ
static_dir = os.path.join(os.path.dirname(__file__), "static")
if os.path.exists(static_dir):
    app.mount("/static", StaticFiles(directory=static_dir), name="static")

# ==================== ì„œë²„ ì‹¤í–‰ ====================

if __name__ == "__main__":
    import uvicorn

    print("=" * 70)
    print("ğŸš€ ì„œë²„ ì‹œì‘")
    print("=" * 70)
    print("ğŸ“ URL: http://localhost:8000")
    print("ğŸ“– API ë¬¸ì„œ: http://localhost:8000/docs")
    print("=" * 70)

    uvicorn.run(app, host="0.0.0.0", port=8000)
