# ë¬¸ì¥ ìˆœì„œ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
# 2025-11-07, ê¹€ë³‘í˜„ ì‘ì„±

import json
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from tqdm import tqdm
import random
from kobert_tokenizer import KoBERTTokenizer
from src.models.sentence_order_model import SentenceOrderPredictor
from src.utils import is_running_in_colab


# ==================== ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ====================


class SentenceOrderDataset(Dataset):
    """ë¬¸ì¥ ìˆœì„œ ì˜ˆì¸¡ ë°ì´í„°ì…‹"""

    def __init__(self, json_path, tokenizer, max_length=128):
        """
        Args:
            json_path: sentence_order_dataset.json ê²½ë¡œ
            tokenizer: KoBERTTokenizer
            max_length: ìµœëŒ€ í† í° ê¸¸ì´
        """
        with open(json_path, "r", encoding="utf-8") as f:
            self.data = json.load(f)

        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        """
        Returns:
            input_ids_list: List of [seq_len] - ê° ë¬¸ì¥ì˜ í† í° ID
            attention_mask_list: List of [seq_len] - ê° ë¬¸ì¥ì˜ ë§ˆìŠ¤í¬
            labels: [num_sentences] - ê° ë¬¸ì¥ì˜ ì˜¬ë°”ë¥¸ ìˆœì„œ

        2025-11-10, ê¹€ë³‘í˜„ ìˆ˜ì • - Subset ì¸ë±ì‹± ì˜¤ë¥˜ ìˆ˜ì •
        """
        # idxë¥¼ ì •ìˆ˜ë¡œ ë³€í™˜ (Subsetì—ì„œ ë„˜ì–´ì˜¬ ìˆ˜ ìˆìŒ)
        idx = int(idx) if not isinstance(idx, int) else idx

        # ì¸ë±ìŠ¤ ë²”ìœ„ ì²´í¬
        if idx < 0 or idx >= len(self.data):
            raise IndexError(
                f"Index {idx} out of range for dataset of size {len(self.data)}"
            )

        item = self.data[idx]

        # itemì´ ë”•ì…”ë„ˆë¦¬ì¸ì§€ í™•ì¸
        if not isinstance(item, dict):
            raise TypeError(f"Expected dict at index {idx}, got {type(item)}")

        sentences = item["shuffled_sentences"]
        labels = item["correct_order"]

        # ë ˆì´ë¸” ë³€í™˜: ìœ„ì¹˜ ë§¤í•‘ â†’ ì„ íƒ ìˆœì„œ
        # 2025-11-17, ê¹€ë³‘í˜„ ìˆ˜ì • - í¬ì¸í„° ë„¤íŠ¸ì›Œí¬ í˜•ì‹ì— ë§ê²Œ ë ˆì´ë¸” ë³€í™˜
        # ë°ì´í„°ì…‹: [6, 7, 1, 2, 5, 0, 4, 3] (0ë²ˆ ë¬¸ì¥ì´ 6ë²ˆì§¸ ìœ„ì¹˜)
        # í•„ìš”í•œ í˜•ì‹: [5, 2, 3, 7, 6, 4, 0, 1] (0ë²ˆì§¸ ìœ„ì¹˜ì— 5ë²ˆ ë¬¸ì¥)
        num_sentences = len(sentences)
        pointer_labels = [0] * num_sentences
        for sentence_idx, position in enumerate(labels):
            pointer_labels[position] = sentence_idx

        # ê° ë¬¸ì¥ì„ í† í°í™”
        input_ids_list = []
        attention_mask_list = []

        for sent in sentences:
            inputs = self.tokenizer(
                sent,
                padding="max_length",
                max_length=self.max_length,
                truncation=True,
                return_tensors="pt",
            )
            input_ids_list.append(inputs["input_ids"].squeeze(0))
            attention_mask_list.append(inputs["attention_mask"].squeeze(0))

        return {
            "input_ids_list": input_ids_list,
            "attention_mask_list": attention_mask_list,
            "labels": torch.tensor(pointer_labels, dtype=torch.long),
        }


# ëª¨ë¸ ì €ì¥ í•¨ìˆ˜
def save_model(model):
    if is_running_in_colab():
        torch.save(
            model.state_dict(), "/content/drive/MyDrive/models/sentence_order_model.pt"
        )
    else:
        torch.save(model.state_dict(), "models/sentence_order_model.pt")


def collate_fn(batch):
    """
    ë°°ì¹˜ ë°ì´í„°ë¥¼ ì ì ˆí•œ í˜•íƒœë¡œ ë³€í™˜ (ê°€ë³€ ê¸¸ì´ ë¬¸ì¥ ì§€ì›)
    2025-11-07, ê¹€ë³‘í˜„ ìˆ˜ì • - ê°€ë³€ ê¸¸ì´ ë¬¸ì¥ ì²˜ë¦¬ ì¶”ê°€
    """
    # ë°°ì¹˜ ë‚´ì—ì„œ ìµœëŒ€ ë¬¸ì¥ ê°œìˆ˜ ì°¾ê¸°
    max_num_sentences = max(len(item["input_ids_list"]) for item in batch)

    # ê° ë¬¸ì¥ë³„ë¡œ ë°°ì¹˜ êµ¬ì„±
    input_ids_batch = []
    attention_mask_batch = []
    labels_batch = []

    for i in range(max_num_sentences):
        # ië²ˆì§¸ ë¬¸ì¥ì´ ìˆëŠ” ìƒ˜í”Œë“¤ë§Œ ìˆ˜ì§‘
        input_ids_list = []
        attention_mask_list = []

        for item in batch:
            if i < len(item["input_ids_list"]):
                # ë¬¸ì¥ì´ ìˆìœ¼ë©´ ì‹¤ì œ ë°ì´í„° ì¶”ê°€
                input_ids_list.append(item["input_ids_list"][i])
                attention_mask_list.append(item["attention_mask_list"][i])
            else:
                # ë¬¸ì¥ì´ ì—†ìœ¼ë©´ íŒ¨ë”© (ëª¨ë‘ 0ì¸ í…ì„œ)
                seq_len = item["input_ids_list"][0].shape[0]
                input_ids_list.append(torch.zeros(seq_len, dtype=torch.long))
                attention_mask_list.append(torch.zeros(seq_len, dtype=torch.long))

        input_ids_batch.append(torch.stack(input_ids_list))
        attention_mask_batch.append(torch.stack(attention_mask_list))

    # ë ˆì´ë¸”ë„ íŒ¨ë”© (-100ì€ loss ê³„ì‚° ì‹œ ë¬´ì‹œë¨)
    for item in batch:
        labels = item["labels"].tolist()
        # max_num_sentencesê¹Œì§€ íŒ¨ë”©
        labels += [-100] * (max_num_sentences - len(labels))
        labels_batch.append(torch.tensor(labels, dtype=torch.long))

    labels = torch.stack(labels_batch)

    return {
        "input_ids_list": input_ids_batch,
        "attention_mask_list": attention_mask_batch,
        "labels": labels,
    }


# ==================== í•™ìŠµ í•¨ìˆ˜ ====================


def train_epoch(
    model, dataloader, optimizer, criterion, device, gradient_accumulation_steps=1
):
    """
    í•œ ì—í¬í¬ í•™ìŠµ
    2025-11-17, ê¹€ë³‘í˜„ ìˆ˜ì • - ë¶€ë¶„ ì •í™•ë„ ë©”íŠ¸ë¦­ ì¶”ê°€
    """
    model.train()
    total_loss = 0
    correct = 0  # ì™„ì „ ì •í™•ë„ (ëª¨ë“  ìŠ¤í… ì¼ì¹˜)
    correct_steps = 0  # ìŠ¤í…ë³„ ì •í™•ë„ (ë¶€ë¶„ ì •ë‹µ)
    total = 0
    total_steps = 0

    progress_bar = tqdm(dataloader, desc="Training")
    optimizer.zero_grad()

    for batch_idx, batch in enumerate(progress_bar):
        input_ids_list = [ids.to(device) for ids in batch["input_ids_list"]]
        attention_mask_list = [mask.to(device) for mask in batch["attention_mask_list"]]
        labels = batch["labels"].to(device)

        # Forward pass
        logits = model(input_ids_list, attention_mask_list)

        # Loss ê³„ì‚°
        # 2025-11-17, ê¹€ë³‘í˜„ ìˆ˜ì • - í¬ì¸í„° ë„¤íŠ¸ì›Œí¬ loss ê³„ì‚° ë°©ì‹ ë³€ê²½
        # logits: [batch_size, num_steps, num_sentences] (ê° ìŠ¤í…ì—ì„œ ì„ íƒí•  ë¬¸ì¥ì˜ í™•ë¥ )
        # labels: [batch_size, num_steps] (ê° ìŠ¤í…ì—ì„œ ì„ íƒí•´ì•¼ í•  ë¬¸ì¥ ì¸ë±ìŠ¤)
        batch_size, num_steps, num_choices = logits.shape

        # ë””ë²„ê¹…: logitsê³¼ labels í™•ì¸
        # 2025-11-17, ê¹€ë³‘í˜„ ìˆ˜ì • - loss ë¬´í•œëŒ€ ë¬¸ì œ ë””ë²„ê¹…
        if batch_idx == 0:
            print(f"\n[DEBUG] logits shape: {logits.shape}")
            print(f"[DEBUG] labels shape: {labels.shape}")
            print(
                f"[DEBUG] logits min/max: {logits.min().item():.2f} / {logits.max().item():.2f}"
            )
            print(f"[DEBUG] labels sample: {labels[0]}")
            print(f"[DEBUG] logits has inf: {torch.isinf(logits).any()}")
            print(f"[DEBUG] logits has nan: {torch.isnan(logits).any()}")

        # ê° ìŠ¤í…ì˜ lossë¥¼ ê³„ì‚°
        loss = criterion(
            logits.view(batch_size * num_steps, num_choices), labels.view(-1)
        )

        # ë””ë²„ê¹…: loss ê°’ í™•ì¸
        if batch_idx == 0:
            print(f"[DEBUG] loss value: {loss.item()}")

        # Gradient Accumulationì„ ìœ„í•´ lossë¥¼ ë‚˜ëˆ”
        loss = loss / gradient_accumulation_steps

        # Backward pass
        loss.backward()

        # Gradient Accumulation: Në²ˆë§ˆë‹¤ ì—…ë°ì´íŠ¸
        if (batch_idx + 1) % gradient_accumulation_steps == 0:
            optimizer.step()
            optimizer.zero_grad()

        # ì •í™•ë„ ê³„ì‚° (ë ˆì´ë¸” -100ì€ ì œì™¸)
        # 2025-11-17, ê¹€ë³‘í˜„ ìˆ˜ì • - ì™„ì „ ì •í™•ë„ + ìŠ¤í…ë³„ ì •í™•ë„ ê³„ì‚°
        predictions = torch.argmax(logits, dim=-1)  # [batch_size, num_steps]
        mask = labels != -100

        # ì™„ì „ ì •í™•ë„: ëª¨ë“  ìŠ¤í…ì—ì„œ ì •í™•íˆ ë§ì¶°ì•¼ ì •ë‹µ
        correct += ((predictions == labels) & mask).all(dim=1).sum().item()
        total += batch_size

        # ìŠ¤í…ë³„ ì •í™•ë„: ê° ìŠ¤í…ë§ˆë‹¤ ë§ì¶˜ ê°œìˆ˜
        correct_steps += ((predictions == labels) & mask).sum().item()
        total_steps += mask.sum().item()

        total_loss += loss.item() * gradient_accumulation_steps
        progress_bar.set_postfix(
            {
                "loss": loss.item() * gradient_accumulation_steps,
                "acc": correct / total if total > 0 else 0,
                "step_acc": correct_steps / total_steps if total_steps > 0 else 0,
            }
        )

    avg_loss = total_loss / len(dataloader)
    accuracy = correct / total if total > 0 else 0
    step_accuracy = correct_steps / total_steps if total_steps > 0 else 0

    return avg_loss, accuracy, step_accuracy


def evaluate(model, dataloader, criterion, device):
    """
    ëª¨ë¸ í‰ê°€
    2025-11-17, ê¹€ë³‘í˜„ ìˆ˜ì • - ë¶€ë¶„ ì •í™•ë„ ë©”íŠ¸ë¦­ ì¶”ê°€
    """
    model.eval()
    total_loss = 0
    correct = 0
    correct_steps = 0
    total = 0
    total_steps = 0

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Evaluating"):
            input_ids_list = [ids.to(device) for ids in batch["input_ids_list"]]
            attention_mask_list = [
                mask.to(device) for mask in batch["attention_mask_list"]
            ]
            labels = batch["labels"].to(device)

            logits = model(input_ids_list, attention_mask_list)

            # Loss ê³„ì‚°
            # 2025-11-17, ê¹€ë³‘í˜„ ìˆ˜ì • - trainê³¼ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ê³„ì‚°
            batch_size, num_steps, num_choices = logits.shape
            loss = criterion(
                logits.view(batch_size * num_steps, num_choices), labels.view(-1)
            )

            # ì •í™•ë„ ê³„ì‚° (ë ˆì´ë¸” -100ì€ ì œì™¸)
            # 2025-11-17, ê¹€ë³‘í˜„ ìˆ˜ì • - ì™„ì „ ì •í™•ë„ + ìŠ¤í…ë³„ ì •í™•ë„ ê³„ì‚°
            predictions = torch.argmax(logits, dim=-1)
            mask = labels != -100

            # ì™„ì „ ì •í™•ë„
            correct += ((predictions == labels) & mask).all(dim=1).sum().item()
            total += batch_size

            # ìŠ¤í…ë³„ ì •í™•ë„
            correct_steps += ((predictions == labels) & mask).sum().item()
            total_steps += mask.sum().item()

            total_loss += loss.item()

    avg_loss = total_loss / len(dataloader) if len(dataloader) > 0 else 0
    accuracy = correct / total if total > 0 else 0
    step_accuracy = correct_steps / total_steps if total_steps > 0 else 0

    return avg_loss, accuracy, step_accuracy


# ==================== ë©”ì¸ í•™ìŠµ ë£¨í”„ ====================


def main():
    print("=" * 70)
    print("ë¬¸ì¥ ìˆœì„œ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ")
    print("=" * 70)

    # í•˜ì´í¼íŒŒë¼ë¯¸í„°
    # 2025-11-07, ê¹€ë³‘í˜„ ìˆ˜ì • - ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•œ ì„¤ì • ì¡°ì •
    BATCH_SIZE = 16  # 8 â†’ 2 (ë©”ëª¨ë¦¬ ë¶€ì¡± ë°©ì§€)
    LEARNING_RATE = 1e-4  # Pointer Network ë ˆì´ì–´ì˜ Learning Rate
    BERT_LR = 2e-5  # 2025-11-17, ê¹€ë³‘í˜„ ìˆ˜ì • - BERT Fine-tuning Learning Rate
    EPOCHS = 20
    MAX_SENTENCES = 12  # ë°ì´í„°ì…‹ì— 12ê°œ ë¬¸ì¥ê¹Œì§€ ìˆìŒ
    MAX_LENGTH = 64  # 128 â†’ 64 (ë¬¸ì¥ì´ ì§§ìœ¼ë¯€ë¡œ ì¤„ì„)
    TRAIN_SPLIT = 0.8
    GRADIENT_ACCUMULATION_STEPS = 4  # ì‹¤ì§ˆì  ë°°ì¹˜ í¬ê¸° = 2 Ã— 4 = 8

    # Early Stopping ì„¤ì •
    # 2025-11-13, ê¹€ë³‘í˜„ ìˆ˜ì • - ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•œ Early Stopping ì¶”ê°€
    EARLY_STOPPING_PATIENCE = 3  # ê²€ì¦ ì •í™•ë„ê°€ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ N epoch í›„ ì¤‘ë‹¨
    RESUME_TRAINING = True  # True: ê¸°ì¡´ ëª¨ë¸ ì´ì–´ì„œ í•™ìŠµ, False: ìƒˆë¡œ ì‹œì‘
    UNFREEZE_BERT = True  # 2025-11-17, ê¹€ë³‘í˜„ ìˆ˜ì • - BERT Unfreeze ì—¬ë¶€

    # Device ì„¤ì •
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"\nâœ… Device: {device}")

    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    print("âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...")
    tokenizer = KoBERTTokenizer.from_pretrained("skt/kobert-base-v1")

    # ë°ì´í„°ì…‹ ë¡œë“œ
    # 2025-11-13, ê¹€ë³‘í˜„ ìˆ˜ì • - ì¼ë¶€ ë°ì´í„°ë§Œ ì‚¬ìš©í•˜ëŠ” ì˜µì…˜ ì¶”ê°€
    print("âœ… ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
    dataset = SentenceOrderDataset(
        "data/sentence_order_dataset.json", tokenizer, max_length=MAX_LENGTH
    )
    print(f"   ì „ì²´ ë°ì´í„°: {len(dataset)}ê°œ")

    # ë°ì´í„° ì¼ë¶€ë§Œ ì‚¬ìš© (í…ŒìŠ¤íŠ¸ìš©)
    USE_SUBSET = False  # True: ì¼ë¶€ë§Œ ì‚¬ìš©, False: ì „ì²´ ì‚¬ìš©
    SUBSET_SIZE = 500  # ì‚¬ìš©í•  ë°ì´í„° ê°œìˆ˜

    if USE_SUBSET and len(dataset) > SUBSET_SIZE:
        # ëœë¤í•˜ê²Œ ì¼ë¶€ ì„ íƒ
        import random

        indices = list(range(len(dataset)))
        random.shuffle(indices)
        selected_indices = indices[:SUBSET_SIZE]
        dataset = torch.utils.data.Subset(dataset, selected_indices)
        print(f"   âš ï¸  ì¼ë¶€ ë°ì´í„°ë§Œ ì‚¬ìš©: {SUBSET_SIZE}ê°œ")

    # Train/Val ë¶„í• 
    train_size = int(len(dataset) * TRAIN_SPLIT)
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(
        dataset, [train_size, val_size]
    )
    print(f"   í•™ìŠµ ë°ì´í„°: {train_size}ê°œ")
    print(f"   ê²€ì¦ ë°ì´í„°: {val_size}ê°œ")

    # DataLoader
    train_loader = DataLoader(
        train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn
    )
    val_loader = DataLoader(
        val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn
    )

    # ëª¨ë¸ ì´ˆê¸°í™”
    print("âœ… ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
    model = SentenceOrderPredictor(
        max_sentences=MAX_SENTENCES, hidden_size=768, dropout=0.1
    ).to(device)

    # ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì—¬ë¶€ í™•ì¸
    # 2025-11-13, ê¹€ë³‘í˜„ ìˆ˜ì • - RESUME_TRAINING ì˜µì…˜ ì¶”ê°€
    import os

    if is_running_in_colab():
        pretrained_model_path = "/content/drive/MyDrive/models/sentence_order_model_best.pt"
    else:
        pretrained_model_path = "models/sentence_order_model_best.pt"
    if RESUME_TRAINING and os.path.exists(pretrained_model_path):
        print(f"   ğŸ”„ ê¸°ì¡´ ëª¨ë¸ ë°œê²¬: {pretrained_model_path}")
        print(f"   ğŸ“¥ ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì¤‘... (ì´ì–´ì„œ í•™ìŠµ)")
        model.load_state_dict(torch.load(pretrained_model_path, map_location=device))
        print(f"   âœ… ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
    else:
        if not RESUME_TRAINING:
            print(f"   ğŸ†• ìƒˆë¡œìš´ ëª¨ë¸ ìƒì„± (RESUME_TRAINING=False)")
        else:
            print(f"   ğŸ†• ìƒˆë¡œìš´ ëª¨ë¸ ìƒì„± (ê¸°ì¡´ ëª¨ë¸ ì—†ìŒ)")

    # BERT Freeze/Unfreeze ì„¤ì •
    # 2025-11-17, ê¹€ë³‘í˜„ ìˆ˜ì • - BERT Unfreeze ì˜µì…˜ ì¶”ê°€
    if not UNFREEZE_BERT:
        # BERTë¥¼ Freeze (ê¸°ì¡´ ë°©ì‹)
        for param in model.bert.parameters():
            param.requires_grad = False
        print(f"   ğŸ”’ BERT Frozen: BERT íŒŒë¼ë¯¸í„°ëŠ” í•™ìŠµí•˜ì§€ ì•ŠìŒ")
    else:
        # BERTë¥¼ Unfreeze (ëª¨ë“  íŒŒë¼ë¯¸í„° í•™ìŠµ)
        for param in model.bert.parameters():
            param.requires_grad = True
        print(f"   ğŸ”“ BERT Unfrozen: BERT íŒŒë¼ë¯¸í„°ë„ í•¨ê»˜ í•™ìŠµ")

    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"   ì „ì²´ íŒŒë¼ë¯¸í„°: {total_params:,}")
    print(f"   í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {trainable_params:,}")

    # Optimizer & Loss
    # 2025-11-17, ê¹€ë³‘í˜„ ìˆ˜ì • - Discriminative Learning Rate ì ìš© (BERT Unfreeze ì‹œ)
    # BERT: ì‘ì€ lr (2e-5), Pointer Network: í° lr (1e-4)
    if UNFREEZE_BERT:
        optimizer = AdamW(
            [
                # BERT ì „ì²´: ì‘ì€ lr
                {"params": model.bert.parameters(), "lr": BERT_LR},
                # Pointer Network ë ˆì´ì–´: í° lr
                {"params": model.sequence_encoder.parameters(), "lr": LEARNING_RATE},
                {"params": model.pointer_decoder.parameters(), "lr": LEARNING_RATE},
            ],
            weight_decay=0.01,
        )
    else:
        # BERTê°€ Frozenì¼ ë•ŒëŠ” Pointer Networkë§Œ í•™ìŠµ
        optimizer = AdamW(
            [
                {"params": model.sequence_encoder.parameters(), "lr": LEARNING_RATE},
                {"params": model.pointer_decoder.parameters(), "lr": LEARNING_RATE},
            ],
            weight_decay=0.01,
        )

    criterion = nn.CrossEntropyLoss(ignore_index=-100)

    # í•™ìŠµ ì •ë³´ ì¶œë ¥
    print("\n" + "=" * 70)
    print("âš™ï¸  í•™ìŠµ ì„¤ì •")
    print("=" * 70)
    print(f"   ğŸ”„ Epochs: {EPOCHS}")
    print(
        f"   ğŸ“¦ Batch Size: {BATCH_SIZE} (ì‹¤ì§ˆì : {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS})"
    )
    print(f"   ğŸ’¾ Gradient Accumulation: {GRADIENT_ACCUMULATION_STEPS} steps")
    print(f"   ğŸ“ Max Length: {MAX_LENGTH} tokens")
    print(f"   ğŸ›‘ Early Stopping: Patience {EARLY_STOPPING_PATIENCE} epochs")
    # 2025-11-17, ê¹€ë³‘í˜„ ìˆ˜ì • - BERT Unfreeze ì •ë³´ ì¶”ê°€
    if UNFREEZE_BERT:
        print(f"   ğŸ”“ BERT Unfreeze: True (LR: {BERT_LR}, Pointer LR: {LEARNING_RATE})")
    else:
        print(f"   ğŸ”’ BERT Frozen: True (Pointer LR: {LEARNING_RATE})")

    # í•™ìŠµ ì‹œì‘
    print("\n" + "=" * 70)
    print("ğŸš€ í•™ìŠµ ì‹œì‘")
    print("=" * 70)

    # ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    # 2025-11-17, ê¹€ë³‘í˜„ ìˆ˜ì • - models í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
    os.makedirs("models", exist_ok=True)

    # Early Stopping ë³€ìˆ˜ ì´ˆê¸°í™”
    # 2025-11-13, ê¹€ë³‘í˜„ ìˆ˜ì • - Early Stopping êµ¬í˜„
    best_val_acc = 0
    best_step_acc = 0
    patience_counter = 0  # ê°œì„ ë˜ì§€ ì•Šì€ ì—°ì† epoch ìˆ˜

    for epoch in range(EPOCHS):
        print(f"\nğŸ“ Epoch {epoch + 1}/{EPOCHS}")

        # í•™ìŠµ
        train_loss, train_acc, train_step_acc = train_epoch(
            model,
            train_loader,
            optimizer,
            criterion,
            device,
            gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
        )
        print(
            f"   Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train Step Acc: {train_step_acc:.4f}"
        )

        # ê²€ì¦
        val_loss, val_acc, val_step_acc = evaluate(model, val_loader, criterion, device)
        print(
            f"   Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val Step Acc: {val_step_acc:.4f}"
        )

        # ìµœê³  ëª¨ë¸ ì €ì¥ ë° Early Stopping ì²´í¬
        # ìŠ¤í… ì •í™•ë„, ì „ì²´ ë¬¸ì¥ì„ ë§ì¶˜ ì •í™•ë„ ë‘˜ ë‹¤ ê³ ë ¤
        if val_step_acc > best_step_acc:
            best_step_acc = val_step_acc
            patience_counter = 0  # ê°œì„ ë˜ì—ˆìœ¼ë¯€ë¡œ ì¹´ìš´í„° ë¦¬ì…‹
            torch.save(model.state_dict(), "models/sentence_order_model_best.pt")
            print(f"   âœ¨ ìµœê³  ëª¨ë¸ ì €ì¥! (Val Step Acc: {val_step_acc:.4f})")
        elif val_acc > best_val_acc:
            best_val_acc = val_acc
            patience_counter = 0  # ê°œì„ ë˜ì—ˆìœ¼ë¯€ë¡œ ì¹´ìš´í„° ë¦¬ì…‹
            if is_running_in_colab():
                torch.save(model.state_dict(), "models/sentence_order_model_best.pt")
            else:
                torch.save(model.state_dict(), "models/sentence_order_model_best.pt")
            print(f"   âœ¨ ìµœê³  ëª¨ë¸ ì €ì¥! (Val Acc: {val_acc:.4f})")
        else:
            patience_counter += 1
            print(
                f"   âš ï¸  ê²€ì¦ ì •í™•ë„ ê°œì„  ì—†ìŒ ({patience_counter}/{EARLY_STOPPING_PATIENCE})"
            )

            # Early Stopping ì¡°ê±´ ì¶©ì¡±
            if patience_counter >= EARLY_STOPPING_PATIENCE:
                print(
                    f"\nğŸ›‘ Early Stopping: {EARLY_STOPPING_PATIENCE} epoch ë™ì•ˆ ê°œì„  ì—†ìŒ"
                )
                print(f"   ìµœê³  ê²€ì¦ ì •í™•ë„: {best_val_acc:.4f}")
                break

    print("\n" + "=" * 70)
    print(f"âœ… í•™ìŠµ ì™„ë£Œ! ìµœê³  ê²€ì¦ ì •í™•ë„: {best_val_acc:.4f}")
    print("=" * 70)

    # ìµœì¢… ëª¨ë¸ ì €ì¥
    torch.save(model.state_dict(), "models/sentence_order_model_final.pt")
    print("âœ… ìµœì¢… ëª¨ë¸ ì €ì¥: models/sentence_order_model_final.pt")


if __name__ == "__main__":
    main()
