# ë¬¸ì¥ ìˆœì„œ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
# 2025-11-07, ê¹€ë³‘í˜„ ì‘ì„±

import json
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from tqdm import tqdm
import random
from kobert_tokenizer import KoBERTTokenizer
from src.models.sentence_order_model import SentenceOrderPredictor


# ==================== ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ====================


class SentenceOrderDataset(Dataset):
    """ë¬¸ì¥ ìˆœì„œ ì˜ˆì¸¡ ë°ì´í„°ì…‹"""

    def __init__(self, json_path, tokenizer, max_length=128):
        """
        Args:
            json_path: sentence_order_dataset.json ê²½ë¡œ
            tokenizer: KoBERTTokenizer
            max_length: ìµœëŒ€ í† í° ê¸¸ì´
        """
        with open(json_path, "r", encoding="utf-8") as f:
            self.data = json.load(f)

        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        """
        Returns:
            input_ids_list: List of [seq_len] - ê° ë¬¸ì¥ì˜ í† í° ID
            attention_mask_list: List of [seq_len] - ê° ë¬¸ì¥ì˜ ë§ˆìŠ¤í¬
            labels: [num_sentences] - ê° ë¬¸ì¥ì˜ ì˜¬ë°”ë¥¸ ìˆœì„œ
        """
        item = self.data[idx]
        sentences = item["shuffled_sentences"]
        labels = item["correct_order"]

        # ê° ë¬¸ì¥ì„ í† í°í™”
        input_ids_list = []
        attention_mask_list = []

        for sent in sentences:
            inputs = self.tokenizer(
                sent,
                padding="max_length",
                max_length=self.max_length,
                truncation=True,
                return_tensors="pt",
            )
            input_ids_list.append(inputs["input_ids"].squeeze(0))
            attention_mask_list.append(inputs["attention_mask"].squeeze(0))

        return {
            "input_ids_list": input_ids_list,
            "attention_mask_list": attention_mask_list,
            "labels": torch.tensor(labels, dtype=torch.long),
        }


def collate_fn(batch):
    """
    ë°°ì¹˜ ë°ì´í„°ë¥¼ ì ì ˆí•œ í˜•íƒœë¡œ ë³€í™˜ (ê°€ë³€ ê¸¸ì´ ë¬¸ì¥ ì§€ì›)
    2025-11-07, ê¹€ë³‘í˜„ ìˆ˜ì • - ê°€ë³€ ê¸¸ì´ ë¬¸ì¥ ì²˜ë¦¬ ì¶”ê°€
    """
    # ë°°ì¹˜ ë‚´ì—ì„œ ìµœëŒ€ ë¬¸ì¥ ê°œìˆ˜ ì°¾ê¸°
    max_num_sentences = max(len(item["input_ids_list"]) for item in batch)

    # ê° ë¬¸ì¥ë³„ë¡œ ë°°ì¹˜ êµ¬ì„±
    input_ids_batch = []
    attention_mask_batch = []
    labels_batch = []

    for i in range(max_num_sentences):
        # ië²ˆì§¸ ë¬¸ì¥ì´ ìˆëŠ” ìƒ˜í”Œë“¤ë§Œ ìˆ˜ì§‘
        input_ids_list = []
        attention_mask_list = []

        for item in batch:
            if i < len(item["input_ids_list"]):
                # ë¬¸ì¥ì´ ìˆìœ¼ë©´ ì‹¤ì œ ë°ì´í„° ì¶”ê°€
                input_ids_list.append(item["input_ids_list"][i])
                attention_mask_list.append(item["attention_mask_list"][i])
            else:
                # ë¬¸ì¥ì´ ì—†ìœ¼ë©´ íŒ¨ë”© (ëª¨ë‘ 0ì¸ í…ì„œ)
                seq_len = item["input_ids_list"][0].shape[0]
                input_ids_list.append(torch.zeros(seq_len, dtype=torch.long))
                attention_mask_list.append(torch.zeros(seq_len, dtype=torch.long))

        input_ids_batch.append(torch.stack(input_ids_list))
        attention_mask_batch.append(torch.stack(attention_mask_list))

    # ë ˆì´ë¸”ë„ íŒ¨ë”© (-100ì€ loss ê³„ì‚° ì‹œ ë¬´ì‹œë¨)
    for item in batch:
        labels = item["labels"].tolist()
        # max_num_sentencesê¹Œì§€ íŒ¨ë”©
        labels += [-100] * (max_num_sentences - len(labels))
        labels_batch.append(torch.tensor(labels, dtype=torch.long))

    labels = torch.stack(labels_batch)

    return {
        "input_ids_list": input_ids_batch,
        "attention_mask_list": attention_mask_batch,
        "labels": labels,
    }


# ==================== í•™ìŠµ í•¨ìˆ˜ ====================


def train_epoch(
    model, dataloader, optimizer, criterion, device, gradient_accumulation_steps=1
):
    """
    í•œ ì—í¬í¬ í•™ìŠµ
    2025-11-07, ê¹€ë³‘í˜„ ìˆ˜ì • - Gradient Accumulation ì¶”ê°€ (ë©”ëª¨ë¦¬ ì ˆì•½)
    """
    model.train()
    total_loss = 0
    correct = 0
    total = 0

    progress_bar = tqdm(dataloader, desc="Training")
    optimizer.zero_grad()

    for batch_idx, batch in enumerate(progress_bar):
        input_ids_list = [ids.to(device) for ids in batch["input_ids_list"]]
        attention_mask_list = [mask.to(device) for mask in batch["attention_mask_list"]]
        labels = batch["labels"].to(device)

        # Forward pass
        logits = model(input_ids_list, attention_mask_list)

        # Loss ê³„ì‚°
        # logits: [batch_size, num_sentences, num_sentences]
        # labels: [batch_size, num_sentences]
        batch_size, num_sentences, _ = logits.shape
        loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))

        # Gradient Accumulationì„ ìœ„í•´ lossë¥¼ ë‚˜ëˆ”
        loss = loss / gradient_accumulation_steps

        # Backward pass
        loss.backward()

        # Gradient Accumulation: Në²ˆë§ˆë‹¤ ì—…ë°ì´íŠ¸
        if (batch_idx + 1) % gradient_accumulation_steps == 0:
            optimizer.step()
            optimizer.zero_grad()

        # ì •í™•ë„ ê³„ì‚° (ë ˆì´ë¸” -100ì€ ì œì™¸)
        predictions = torch.argmax(logits, dim=-1)
        mask = labels != -100
        correct += ((predictions == labels) & mask).all(dim=1).sum().item()
        total += batch_size

        total_loss += loss.item() * gradient_accumulation_steps
        progress_bar.set_postfix(
            {
                "loss": loss.item() * gradient_accumulation_steps,
                "acc": correct / total if total > 0 else 0,
            }
        )

    avg_loss = total_loss / len(dataloader)
    accuracy = correct / total if total > 0 else 0

    return avg_loss, accuracy


def evaluate(model, dataloader, criterion, device):
    """
    ëª¨ë¸ í‰ê°€
    2025-11-07, ê¹€ë³‘í˜„ ìˆ˜ì • - ì •í™•ë„ ê³„ì‚° ì‹œ -100 ë ˆì´ë¸” ì œì™¸
    """
    model.eval()
    total_loss = 0
    correct = 0
    total = 0

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Evaluating"):
            input_ids_list = [ids.to(device) for ids in batch["input_ids_list"]]
            attention_mask_list = [
                mask.to(device) for mask in batch["attention_mask_list"]
            ]
            labels = batch["labels"].to(device)

            logits = model(input_ids_list, attention_mask_list)

            batch_size = logits.size(0)
            loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))

            # ì •í™•ë„ ê³„ì‚° (ë ˆì´ë¸” -100ì€ ì œì™¸)
            predictions = torch.argmax(logits, dim=-1)
            mask = labels != -100
            correct += ((predictions == labels) & mask).all(dim=1).sum().item()
            total += batch_size
            total_loss += loss.item()

    avg_loss = total_loss / len(dataloader) if len(dataloader) > 0 else 0
    accuracy = correct / total if total > 0 else 0

    return avg_loss, accuracy


# ==================== ë©”ì¸ í•™ìŠµ ë£¨í”„ ====================


def main():
    print("=" * 70)
    print("ë¬¸ì¥ ìˆœì„œ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ")
    print("=" * 70)

    # í•˜ì´í¼íŒŒë¼ë¯¸í„°
    # 2025-11-07, ê¹€ë³‘í˜„ ìˆ˜ì • - ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•œ ì„¤ì • ì¡°ì •
    BATCH_SIZE = 2  # 8 â†’ 2 (ë©”ëª¨ë¦¬ ë¶€ì¡± ë°©ì§€)
    LEARNING_RATE = 2e-5
    EPOCHS = 20  # 10 â†’ 20 (ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ ì¦ê°€)
    MAX_SENTENCES = 12  # ë°ì´í„°ì…‹ì— 12ê°œ ë¬¸ì¥ê¹Œì§€ ìˆìŒ
    MAX_LENGTH = 64  # 128 â†’ 64 (ë¬¸ì¥ì´ ì§§ìœ¼ë¯€ë¡œ ì¤„ì„)
    TRAIN_SPLIT = 0.8
    GRADIENT_ACCUMULATION_STEPS = 4  # ì‹¤ì§ˆì  ë°°ì¹˜ í¬ê¸° = 2 Ã— 4 = 8

    # Device ì„¤ì •
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"\nâœ… Device: {device}")

    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    print("âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...")
    tokenizer = KoBERTTokenizer.from_pretrained("skt/kobert-base-v1")

    # ë°ì´í„°ì…‹ ë¡œë“œ
    print("âœ… ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
    dataset = SentenceOrderDataset(
        "data/sentence_order_dataset.json", tokenizer, max_length=MAX_LENGTH
    )
    print(f"   ì „ì²´ ë°ì´í„°: {len(dataset)}ê°œ")

    # Train/Val ë¶„í• 
    train_size = int(len(dataset) * TRAIN_SPLIT)
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(
        dataset, [train_size, val_size]
    )
    print(f"   í•™ìŠµ ë°ì´í„°: {train_size}ê°œ")
    print(f"   ê²€ì¦ ë°ì´í„°: {val_size}ê°œ")

    # DataLoader
    train_loader = DataLoader(
        train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn
    )
    val_loader = DataLoader(
        val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn
    )

    # ëª¨ë¸ ì´ˆê¸°í™”
    print("âœ… ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
    model = SentenceOrderPredictor(
        max_sentences=MAX_SENTENCES, hidden_size=768, dropout=0.1
    ).to(device)

    # ê¸°ì¡´ ëª¨ë¸ì´ ìˆìœ¼ë©´ ë¡œë“œ (ì´ì–´ì„œ í•™ìŠµ)
    # 2025-11-07, ê¹€ë³‘í˜„ ìˆ˜ì • - ì´ì–´ì„œ í•™ìŠµ ê¸°ëŠ¥ ì¶”ê°€
    import os

    pretrained_model_path = "models/sentence_order_model_best.pt"
    if os.path.exists(pretrained_model_path):
        print(f"   ğŸ”„ ê¸°ì¡´ ëª¨ë¸ ë°œê²¬: {pretrained_model_path}")
        print(f"   ğŸ“¥ ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì¤‘... (ì´ì–´ì„œ í•™ìŠµ)")
        model.load_state_dict(torch.load(pretrained_model_path, map_location=device))
        print(f"   âœ… ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
    else:
        print(f"   ğŸ†• ìƒˆë¡œìš´ ëª¨ë¸ ìƒì„±")

    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"   ì „ì²´ íŒŒë¼ë¯¸í„°: {total_params:,}")
    print(f"   í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {trainable_params:,}")

    # Optimizer & Loss
    # 2025-11-07, ê¹€ë³‘í˜„ ìˆ˜ì • - Discriminative Learning Rate ì ìš©
    # í•˜ìœ„ ë ˆì´ì–´ëŠ” ì‘ì€ lr, ìƒìœ„ ë ˆì´ì–´ì™€ ìƒˆ ë ˆì´ì–´ëŠ” í° lr
    optimizer = AdamW(
        [
            # BERT ì„ë² ë”© & í•˜ìœ„ ë ˆì´ì–´ (0-5): ë§¤ìš° ì‘ì€ lr
            {"params": model.bert.embeddings.parameters(), "lr": 1e-6},
            {"params": model.bert.encoder.layer[:6].parameters(), "lr": 5e-6},
            # BERT ìƒìœ„ ë ˆì´ì–´ (6-11): ì¤‘ê°„ lr
            {"params": model.bert.encoder.layer[6:].parameters(), "lr": 1e-5},
            {"params": model.bert.pooler.parameters(), "lr": 1e-5},
            # ìƒˆë¡œ ì¶”ê°€ëœ ë ˆì´ì–´: í° lr
            {"params": model.sentence_attention.parameters(), "lr": LEARNING_RATE},
            {"params": model.classifier.parameters(), "lr": LEARNING_RATE},
        ],
        weight_decay=0.01,
    )

    criterion = nn.CrossEntropyLoss(ignore_index=-100)

    # í•™ìŠµ ì •ë³´ ì¶œë ¥
    print("\n" + "=" * 70)
    print("âš™ï¸  í•™ìŠµ ì„¤ì •")
    print("=" * 70)
    print(f"   ğŸ“Š Learning Rate ì „ëµ: Discriminative")
    print(f"      - BERT ì„ë² ë”© & í•˜ìœ„ ë ˆì´ì–´ (0-5): 1e-6 ~ 5e-6")
    print(f"      - BERT ìƒìœ„ ë ˆì´ì–´ (6-11): 1e-5")
    print(f"      - ìƒˆ ë ˆì´ì–´ (Attention, Classifier): {LEARNING_RATE}")
    print(f"   ğŸ¯ Weight Decay: 0.01 (L2 ì •ê·œí™”)")
    print(f"   ğŸ”„ Epochs: {EPOCHS}")
    print(
        f"   ğŸ“¦ Batch Size: {BATCH_SIZE} (ì‹¤ì§ˆì : {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS})"
    )
    print(f"   ğŸ’¾ Gradient Accumulation: {GRADIENT_ACCUMULATION_STEPS} steps")
    print(f"   ğŸ“ Max Length: {MAX_LENGTH} tokens")

    # í•™ìŠµ ì‹œì‘
    print("\n" + "=" * 70)
    print("ğŸš€ í•™ìŠµ ì‹œì‘")
    print("=" * 70)

    best_val_acc = 0
    for epoch in range(EPOCHS):
        print(f"\nğŸ“ Epoch {epoch + 1}/{EPOCHS}")

        # í•™ìŠµ
        train_loss, train_acc = train_epoch(
            model,
            train_loader,
            optimizer,
            criterion,
            device,
            gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
        )
        print(f"   Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}")

        # ê²€ì¦
        val_loss, val_acc = evaluate(model, val_loader, criterion, device)
        print(f"   Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")

        # ìµœê³  ëª¨ë¸ ì €ì¥
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), "models/sentence_order_model_best.pt")
            print(f"   âœ¨ ìµœê³  ëª¨ë¸ ì €ì¥! (Val Acc: {val_acc:.4f})")

    print("\n" + "=" * 70)
    print(f"âœ… í•™ìŠµ ì™„ë£Œ! ìµœê³  ê²€ì¦ ì •í™•ë„: {best_val_acc:.4f}")
    print("=" * 70)

    # ìµœì¢… ëª¨ë¸ ì €ì¥
    torch.save(model.state_dict(), "models/sentence_order_model_final.pt")
    print("âœ… ìµœì¢… ëª¨ë¸ ì €ì¥: models/sentence_order_model_final.pt")


if __name__ == "__main__":
    main()
