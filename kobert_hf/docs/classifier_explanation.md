# 문장 순서 예측 분류기 구조 설명
2025-11-07, 김병현 작성

## 현재 구조

```python
self.classifier = nn.Sequential(
    nn.Linear(hidden_size, hidden_size // 2),    # 768 → 384
    nn.ReLU(),                                    # 활성화 함수
    nn.Dropout(dropout),                          # 과적합 방지
    nn.Linear(hidden_size // 2, max_sentences)    # 384 → 6
)
```

## 각 레이어의 역할

### 1️⃣ `nn.Linear(768, 384)` - 차원 축소 레이어

**입력**: 768차원 (KoBERT의 hidden_size)
**출력**: 384차원 (절반으로 축소)

**이유**:
- KoBERT의 출력(768차원)은 일반적인 언어 표현에 최적화되어 있음
- 우리의 태스크(순서 예측)는 768차원 전부가 필요 없을 수 있음
- **차원을 줄여서 파라미터 수를 감소** → 학습 속도 향상, 과적합 방지
- 중간 표현을 학습할 수 있는 여지를 줌

**장점**:
```
파라미터 수 비교:
- 직접 연결 (768→6): 768 × 6 = 4,608개
- 2단계 (768→384→6): (768×384) + (384×6) = 295,296 + 2,304 = 297,600개

→ 더 많은 파라미터로 더 복잡한 패턴을 학습 가능!
```

**단점**:
- 파라미터가 많아서 작은 데이터셋에서는 과적합 위험


### 2️⃣ `nn.ReLU()` - 활성화 함수

**수식**: `f(x) = max(0, x)`

**이유**:
- **비선형성 추가**: 선형 변환만으로는 복잡한 관계를 학습 못함
- ReLU는 가장 많이 쓰이는 활성화 함수 (빠르고 효과적)

**예시**:
```python
입력: [-1, 0, 1, 2]
출력: [0, 0, 1, 2]  # 음수는 0으로
```

**ReLU가 없다면**:
```
Linear(768, 384) + Linear(384, 6)
= Linear(768, 6)과 동일  # 그냥 행렬곱 2번 = 행렬곱 1번과 같음
→ 깊게 쌓는 의미가 없음!
```


### 3️⃣ `nn.Dropout(0.1)` - 과적합 방지

**동작 방식**: 학습 시 10%의 뉴런을 랜덤하게 끔

**이유**:
- **과적합(Overfitting) 방지**: 특정 뉴런에 의존하지 않게 함
- 학습 데이터에만 맞는 모델이 아니라 일반화된 모델을 만듦

**시각화**:
```
학습 시:
[1.2, 0.5, -0.3, 0.8] → [1.2, 0, -0.3, 0.8]  # 10% 확률로 0
                        [0, 0.5, -0.3, 0.8]   # 다른 뉴런 선택

추론 시:
[1.2, 0.5, -0.3, 0.8] → 그대로 사용
```


### 4️⃣ `nn.Linear(384, 6)` - 최종 분류 레이어

**입력**: 384차원
**출력**: 6차원 (각 문장이 0~5번째 위치에 있을 확률)

**출력 해석**:
```python
예시 출력: [0.1, 2.3, -0.5, 1.2, 0.8, -1.0]
           ↓ (Softmax 적용)
확률:      [5%, 55%, 3%, 18%, 12%, 2%]

→ 이 문장은 1번째 위치(55%)에 있을 가능성이 가장 높음!
```


## 🎯 전체 흐름

```
KoBERT 출력 (768차원)
    ↓
[Linear 768→384] : 중요한 특징만 추출
    ↓
[ReLU] : 비선형 변환
    ↓
[Dropout] : 과적합 방지
    ↓
[Linear 384→6] : 각 위치의 점수 출력
    ↓
[CrossEntropyLoss] : 정답과 비교하여 학습
```


## 🔀 다른 가능한 구조들

### 단순한 버전 (추천 - 데이터가 적을 때)
```python
self.classifier = nn.Linear(768, max_sentences)
# 장점: 간단, 빠름, 과적합 위험 낮음
# 단점: 표현력 부족
```

### 더 깊은 버전 (데이터가 많을 때)
```python
self.classifier = nn.Sequential(
    nn.Linear(768, 512),
    nn.ReLU(),
    nn.Dropout(0.1),
    nn.Linear(512, 256),
    nn.ReLU(),
    nn.Dropout(0.1),
    nn.Linear(256, max_sentences)
)
# 장점: 더 복잡한 패턴 학습 가능
# 단점: 학습이 어려움, 과적합 위험 높음
```

### Residual Connection 버전 (고급)
```python
self.fc1 = nn.Linear(768, 768)
self.fc2 = nn.Linear(768, max_sentences)

def forward(self, x):
    residual = x
    x = self.fc1(x)
    x = F.relu(x) + residual  # Skip connection
    x = self.fc2(x)
    return x
# 장점: 깊은 네트워크에서 gradient flow 개선
```


## 💡 현재 구조를 선택한 이유

1. **적당한 복잡도**: 너무 단순하지도, 복잡하지도 않음
2. **검증된 구조**: BERT Fine-tuning에서 자주 쓰이는 패턴
3. **중간 데이터셋 크기에 적합**: 1000개 정도의 데이터에 적절
4. **과적합 방지**: Dropout으로 일반화 성능 향상

## 📊 성능이 안 나올 때 시도할 것

1. **더 단순하게**: `nn.Linear(768, 6)` 만 사용
2. **Dropout 비율 조정**: 0.1 → 0.3으로 증가
3. **레이어 추가**: 중간에 레이어 하나 더 추가
4. **다른 활성화 함수**: ReLU → GELU, LeakyReLU
