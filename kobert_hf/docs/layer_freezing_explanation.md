# Fine-tuning ì‹œ ë ˆì´ì–´ë³„ í•™ìŠµ ì „ëµ
2025-11-07, ê¹€ë³‘í˜„ ì‘ì„±

## ì™œ ê¹Šì€ ë ˆì´ì–´ëŠ” ëœ í•™ìŠµì‹œí‚¤ë‚˜?

### BERTì˜ ë ˆì´ì–´ êµ¬ì¡°
```
Layer 0 (ì…ë ¥ì¸µ)    â†’ ì¼ë°˜ì ì¸ ë‹¨ì–´ í‘œí˜„ (ë²”ìš©ì )
Layer 1-3          â†’ í˜•íƒœì†Œ, êµ¬ë¬¸ ì •ë³´
Layer 4-8          â†’ ë¬¸ë§¥, ì˜ë¯¸ ì •ë³´
Layer 9-11 (ì¶œë ¥ì¸µ) â†’ íƒœìŠ¤í¬ íŠ¹í™” ì •ë³´
```

### í•µì‹¬ ê°œë…
1. **í•˜ìœ„ ë ˆì´ì–´**: ì´ë¯¸ ì¢‹ì€ ì¼ë°˜ì  í‘œí˜„ì„ í•™ìŠµí–ˆìŒ â†’ í¬ê²Œ ë°”ê¿€ í•„ìš” ì—†ìŒ
2. **ìƒìœ„ ë ˆì´ì–´**: ìš°ë¦¬ì˜ íƒœìŠ¤í¬ì— ë§ê²Œ ì¡°ì • í•„ìš” â†’ ë§ì´ í•™ìŠµ

## ğŸ¯ 3ê°€ì§€ ì „ëµ

### ì „ëµ 1: **ë ˆì´ì–´ Freeze** (ê°€ì¥ ê°„ë‹¨)

í•˜ìœ„ ë ˆì´ì–´ë¥¼ ì™„ì „íˆ ê³ ì • (í•™ìŠµ ì•ˆ í•¨)

```python
# BERTì˜ ì²˜ìŒ 6ê°œ ë ˆì´ì–´ freeze
for param in model.bert.encoder.layer[:6].parameters():
    param.requires_grad = False
```

**ì¥ì **:
- í•™ìŠµ ì†ë„ ë¹ ë¦„ (íŒŒë¼ë¯¸í„° ì¤„ì–´ë“¦)
- ê³¼ì í•© ë°©ì§€
- ë©”ëª¨ë¦¬ ì ˆì•½

**ë‹¨ì **:
- í‘œí˜„ë ¥ ì œí•œë  ìˆ˜ ìˆìŒ


### ì „ëµ 2: **Discriminative Learning Rate** (ì¶”ì²œ!)

ë ˆì´ì–´ë³„ë¡œ ë‹¤ë¥¸ learning rate ì ìš©

```python
# í•˜ìœ„ ë ˆì´ì–´: ì‘ì€ lr (ê±°ì˜ ì•ˆ ë°”ë€œ)
# ìƒìœ„ ë ˆì´ì–´: í° lr (ë§ì´ ë°”ë€œ)
# ë¶„ë¥˜ê¸°: ê°€ì¥ í° lr (ìƒˆë¡œ í•™ìŠµ)

optimizer = AdamW([
    {'params': model.bert.embeddings.parameters(), 'lr': 1e-6},
    {'params': model.bert.encoder.layer[:6].parameters(), 'lr': 1e-6},
    {'params': model.bert.encoder.layer[6:].parameters(), 'lr': 1e-5},
    {'params': model.classifier.parameters(), 'lr': 2e-5}
])
```

**ì¥ì **:
- ë¯¸ì„¸ ì¡°ì • ê°€ëŠ¥
- ê³¼ì í•© ë°©ì§€í•˜ë©´ì„œ í‘œí˜„ë ¥ ìœ ì§€

**ë‹¨ì **:
- í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ í•„ìš”


### ì „ëµ 3: **Gradual Unfreezing** (ê³ ê¸‰)

ì²˜ìŒì—ëŠ” freezeí•˜ê³  ì ì°¨ í’€ì–´ì¤Œ

```python
# Epoch 1-2: ë¶„ë¥˜ê¸°ë§Œ í•™ìŠµ
# Epoch 3-4: ìƒìœ„ ë ˆì´ì–´ í•™ìŠµ
# Epoch 5+: ì „ì²´ í•™ìŠµ
```

**ì¥ì **:
- ì•ˆì •ì  í•™ìŠµ
- ì¢‹ì€ ì„±ëŠ¥

**ë‹¨ì **:
- êµ¬í˜„ ë³µì¡
- í•™ìŠµ ì‹œê°„ ê¹€


## ğŸ“Š ë°ì´í„° í¬ê¸°ë³„ ì¶”ì²œ

| ë°ì´í„° í¬ê¸° | ì¶”ì²œ ì „ëµ |
|------------|----------|
| < 100ê°œ | ì „ëµ 1: í•˜ìœ„ 9ê°œ ë ˆì´ì–´ freeze |
| 100-1000ê°œ | ì „ëµ 2: Discriminative LR (í˜„ì¬ ìƒí™©) |
| > 1000ê°œ | ì „ëµ 3 ë˜ëŠ” ì „ì²´ í•™ìŠµ |


## ğŸ”¥ í˜„ì¬ ì½”ë“œ ë¬¸ì œì 

```python
# í˜„ì¬ ì½”ë“œ
optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)
```

**ë¬¸ì œ**:
- BERTì˜ ëª¨ë“  ë ˆì´ì–´ê°€ ë™ì¼í•œ learning rate (2e-5)
- í•˜ìœ„ ë ˆì´ì–´ë„ ë§ì´ ë³€ê²½ë  ìˆ˜ ìˆìŒ
- ê³¼ì í•© ìœ„í—˜ ì¦ê°€


## âœ… ê°œì„ ëœ ì½”ë“œ

```python
# ê°œì„ ì•ˆ 1: ë ˆì´ì–´ Freeze
for param in model.bert.encoder.layer[:8].parameters():
    param.requires_grad = False

optimizer = AdamW(model.parameters(), lr=2e-5)
```

```python
# ê°œì„ ì•ˆ 2: Discriminative Learning Rate (ì¶”ì²œ!)
optimizer = AdamW([
    # BERT ì„ë² ë”© & í•˜ìœ„ ë ˆì´ì–´: ë§¤ìš° ì‘ì€ lr
    {'params': model.bert.embeddings.parameters(), 'lr': 1e-6},
    {'params': model.bert.encoder.layer[:6].parameters(), 'lr': 5e-6},

    # BERT ìƒìœ„ ë ˆì´ì–´: ì¤‘ê°„ lr
    {'params': model.bert.encoder.layer[6:].parameters(), 'lr': 1e-5},

    # Pooler: ì¤‘ê°„ lr
    {'params': model.bert.pooler.parameters(), 'lr': 1e-5},

    # Attention & ë¶„ë¥˜ê¸°: í° lr (ìƒˆë¡œ ì¶”ê°€ëœ ë ˆì´ì–´)
    {'params': model.sentence_attention.parameters(), 'lr': 2e-5},
    {'params': model.classifier.parameters(), 'lr': 2e-5}
], weight_decay=0.01)
```


## ğŸ“ í•™ìŠµ íŒ

1. **ì²˜ìŒì—ëŠ” ì „ëµ 1ë¡œ ì‹œì‘** (ë¹ ë¥´ê²Œ í…ŒìŠ¤íŠ¸)
2. **ì„±ëŠ¥ ë¶€ì¡±í•˜ë©´ ì „ëµ 2 ì ìš©** (ë” ì¢‹ì€ ì„±ëŠ¥)
3. **Validation accuracy ëª¨ë‹ˆí„°ë§** (ê³¼ì í•© ì²´í¬)
